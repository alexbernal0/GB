{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a564927c",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "what are some quanititative tests or measure we can do on this dataset to try and understand which modeling type or method would be best? the target variable is 12m, and the features all all the other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a918a440",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "To determine the best modeling type or method for your dataset, you can perform several quantitative tests and measures. Here are some steps you can take:\n",
    "\n",
    "1. **Descriptive Statistics**: \n",
    "   - Calculate the mean, median, standard deviation, and range for each feature. This will help you understand the distribution and variability of the data.\n",
    "\n",
    "2. **Correlation Analysis**:\n",
    "   - Compute the correlation matrix to see how each feature correlates with the target variable (12m). This can help identify which features are most strongly related to the target.\n",
    "\n",
    "3. **Feature Importance**:\n",
    "   - Use techniques like Random Forest or Gradient Boosting to assess feature importance. This can help identify which features contribute most to predicting the target variable.\n",
    "\n",
    "4. **Multicollinearity Check**:\n",
    "   - Check for multicollinearity among features using Variance Inflation Factor (VIF). High multicollinearity can affect the performance of linear models.\n",
    "\n",
    "5. **Dimensionality Reduction**:\n",
    "   - Apply techniques like PCA (Principal Component Analysis) to reduce dimensionality and see if the reduced features can still explain the variance in the target variable.\n",
    "\n",
    "6. **Distribution Analysis**:\n",
    "   - Plot histograms or density plots for each feature to understand their distributions. This can help in deciding whether transformations are needed.\n",
    "\n",
    "7. **Outlier Detection**:\n",
    "   - Identify and analyze outliers in the dataset. Outliers can significantly affect model performance, especially in linear models.\n",
    "\n",
    "8. **Model Selection Tests**:\n",
    "   - Split the data into training and testing sets and try different models (e.g., Linear Regression, Decision Trees, SVM, etc.) to see which performs best based on metrics like RMSE, MAE, or R-squared.\n",
    "\n",
    "9. **Cross-Validation**:\n",
    "   - Use cross-validation to assess the stability and performance of different models. This helps in understanding how well the model generalizes to unseen data.\n",
    "\n",
    "10. **Residual Analysis**:\n",
    "    - Analyze the residuals of the models to check for patterns. Ideally, residuals should be randomly distributed.\n",
    "\n",
    "Let's start by loading the dataset and performing some initial descriptive statistics and correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1711bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'GB4_12m Ready to Model.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "descriptive_stats = df.describe()\n",
    "print(descriptive_stats)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23be9aa",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here are the initial results from the dataset analysis:\n",
    "\n",
    "1. **Descriptive Statistics**:\n",
    "   - The dataset contains 112 rows for each feature.\n",
    "   - The mean, standard deviation, minimum, and maximum values for each feature have been calculated. This provides a basic understanding of the distribution and variability of the data.\n",
    "\n",
    "2. **Correlation Matrix**:\n",
    "   - The correlation matrix shows how each feature correlates with the target variable (12m) and with each other. This can help identify which features are most strongly related to the target and if there are any multicollinearity issues.\n",
    "\n",
    "{outputs_dict['4793a3b5']}\n",
    "{outputs_dict['11253cae']}\n",
    "{outputs_dict['7963242c']}\n",
    "\n",
    "Next steps could include feature importance analysis, multicollinearity checks, and trying different models to see which performs best. Let me know if you want to proceed with any specific analysis or modeling approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf2557",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c19cf",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Let's proceed with the following steps:\n",
    "\n",
    "1. **Feature Importance Analysis**: We'll use a Random Forest model to assess feature importance. This will help us understand which features contribute most to predicting the target variable.\n",
    "\n",
    "2. **Multicollinearity Check**: We'll calculate the Variance Inflation Factor (VIF) for each feature to check for multicollinearity.\n",
    "\n",
    "3. **Model Selection Tests**: We'll split the data into training and testing sets and try different models to see which performs best based on metrics like RMSE, MAE, or R-squared.\n",
    "\n",
    "Let's start with the feature importance analysis using a Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80bc5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df.drop(columns=['12m'])\n",
    "y = df['12m']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Plot feature importances\n",
    "features = X.columns\n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=feature_importances[indices], y=features[indices])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6f4b8e",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here is the plot showing the feature importances as determined by the Random Forest model:\n",
    "\n",
    "{image_urls_dict['33ab67fa']}\n",
    "\n",
    "This plot helps us understand which features are most influential in predicting the target variable (12m). Next, let's proceed with checking for multicollinearity using the Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data['Feature'] = X.columns\n",
    "vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460ffc1",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here are the Variance Inflation Factor (VIF) values for each feature:\n",
    "\n",
    "{outputs_dict['7ad0d23c']}\n",
    "\n",
    "- A VIF value greater than 10 indicates high multicollinearity, which can be problematic for linear models. In this case, the \"Bank Prime Loan Rate (Percent)\" and \"US High Yield CCC or Below Option-Adjusted Spread (Percent)\" have high VIF values, suggesting multicollinearity issues.\n",
    "\n",
    "Next, let's proceed with model selection tests by trying different models and evaluating their performance. We'll start with Linear Regression, Decision Tree, and Support Vector Machine (SVM) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Support Vector Machine': SVR()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'MSE': mse, 'R2': r2}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4bfd0",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e2b44",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Alright, I've stopped the process. If you have any specific questions or need further analysis, feel free to let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda857a0",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "ok based on the results above what are the top 5 model methods that would perform best based on the metrics of this data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7712bf5",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Based on the results from the model evaluation, here are the top models and their performance metrics:\n",
    "\n",
    "1. **Decision Tree**:\n",
    "   - Mean Squared Error (MSE): 65.16\n",
    "   - R-squared (R²): 0.69\n",
    "\n",
    "2. **Linear Regression**:\n",
    "   - Mean Squared Error (MSE): 142.32\n",
    "   - R-squared (R²): 0.33\n",
    "\n",
    "3. **Support Vector Machine (SVM)**:\n",
    "   - Mean Squared Error (MSE): 211.29\n",
    "   - R-squared (R²): 0.01\n",
    "\n",
    "From the models tested, the Decision Tree model performed the best with the lowest MSE and the highest R² value, indicating it explains a significant portion of the variance in the target variable. Linear Regression also performed reasonably well, but not as well as the Decision Tree. The SVM model did not perform well in this case.\n",
    "\n",
    "To explore further, you might consider trying additional models such as Random Forest, Gradient Boosting, or XGBoost, which often perform well in regression tasks. Let me know if you want to proceed with testing any of these models!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d32706",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "ok build a model pipeline to predict the 12m target variable with all the other columns as features using Decision Tree, linear regresssion, support vector machines and a ensemble of the 3, show me at least 3 measures of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create pipelines for each model\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('lr', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline_dt = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('dt', DecisionTreeRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "pipeline_svr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svr', SVR())\n",
    "])\n",
    "\n",
    "# Create an ensemble model using VotingRegressor\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lr', pipeline_lr),\n",
    "    ('dt', pipeline_dt),\n",
    "    ('svr', pipeline_svr)\n",
    "])\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Linear Regression': pipeline_lr,\n",
    "    'Decision Tree': pipeline_dt,\n",
    "    'Support Vector Machine': pipeline_svr,\n",
    "    'Ensemble': ensemble\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mae = np.mean(np.abs(y_test - y_pred))\n",
    "    results[name] = {'MSE': mse, 'R2': r2, 'MAE': mae}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decf819",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here are the results of the model pipeline evaluation for predicting the 12m target variable:\n",
    "\n",
    "{outputs_dict['7620d2a7']}\n",
    "\n",
    "- **Decision Tree**:\n",
    "  - Mean Squared Error (MSE): 65.16\n",
    "  - R-squared (R²): 0.69\n",
    "  - Mean Absolute Error (MAE): 5.54\n",
    "\n",
    "- **Linear Regression**:\n",
    "  - Mean Squared Error (MSE): 142.32\n",
    "  - R-squared (R²): 0.33\n",
    "  - Mean Absolute Error (MAE): 9.55\n",
    "\n",
    "- **Support Vector Machine (SVM)**:\n",
    "  - Mean Squared Error (MSE): 199.57\n",
    "  - R-squared (R²): 0.06\n",
    "  - Mean Absolute Error (MAE): 10.60\n",
    "\n",
    "- **Ensemble (Voting Regressor)**:\n",
    "  - Mean Squared Error (MSE): 105.28\n",
    "  - R-squared (R²): 0.51\n",
    "  - Mean Absolute Error (MAE): 7.59\n",
    "\n",
    "The Decision Tree model performed the best individually, but the Ensemble model, which combines the predictions of all three models, also showed strong performance with a good balance of MSE, R², and MAE. Let me know if you need further analysis or adjustments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a092418",
   "metadata": {},
   "source": [
    "**user_message:**\n",
    "\n",
    "show me some visuals of the accuracy measure for the ensemble model please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16750a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predict using the ensemble model\n",
    "ensemble.fit(X_train, y_train)\n",
    "y_pred_ensemble = ensemble.predict(X_test)\n",
    "\n",
    "# Plotting the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=y_test, y=y_pred_ensemble, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.title('Actual vs Predicted Values (Ensemble Model)')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the residuals\n",
    "residuals = y_test - y_pred_ensemble\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.title('Residuals Distribution (Ensemble Model)')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b90ec12",
   "metadata": {},
   "source": [
    "**ai_message:**\n",
    "\n",
    "Here are the visuals for the accuracy measures of the Ensemble model:\n",
    "\n",
    "1. **Actual vs Predicted Values**: This scatter plot shows the relationship between the actual and predicted values. The closer the points are to the diagonal line, the better the model's predictions.\n",
    "\n",
    "   {image_urls_dict['11383bca']}\n",
    "\n",
    "2. **Residuals Distribution**: This histogram shows the distribution of residuals (the differences between actual and predicted values). Ideally, the residuals should be normally distributed around zero.\n",
    "\n",
    "   {image_urls_dict['0f6c5a9a']}\n",
    "\n",
    "These visuals help in understanding how well the Ensemble model is performing in terms of accuracy and error distribution. Let me know if you need further analysis or any other information!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}